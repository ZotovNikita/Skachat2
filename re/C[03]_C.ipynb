{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDtGOPjTIHVU"
      },
      "source": [
        "#Модуль А"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efzpNJ3dJod6"
      },
      "source": [
        "С помощью pandas.read_csv был прочитан предоставленный в задании датасет, даты были обработаны с помощью parse_dates, sep='|', т.к. такое разделение в основном файле"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wnlR6hSKE4K"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BS4cPKV9KE-a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "f88656e4-ea90-4f51-df13-f447017f439c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-99482a91d9ec>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'albums.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'|'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'release_date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'albums.csv'"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('albums.csv', delimiter='|', parse_dates=['release_date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZtr_b1hL_15"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNwYye5DL_4R"
      },
      "outputs": [],
      "source": [
        "df.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnLLt69eL_cI"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppKvfR6OL3XZ"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTCv_1XJL3uw"
      },
      "outputs": [],
      "source": [
        "df['t_name0'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiEQ9kqDMwzP"
      },
      "outputs": [],
      "source": [
        "df = df.drop(columns=['id', 'Unnamed: 0', 'name', 't_name0', 't_name1', 't_name2'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKt9CjioN2zF"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4TJkRZoObOP"
      },
      "outputs": [],
      "source": [
        "df['artists'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqoWiPYtLT-C"
      },
      "source": [
        "Удалим ненужные столбцы: Unnamed:0, и id являются уникальными индетификаторами, тоже самое можно сказать про name - для разделения на категории слишком много уникальных значений, а для языкового анализа физически не хватит времени на демоэкзамене. Тоже можно сказать про t_name. Имеет смысл рассмотреть artists, потому что популярность артиста определенно может влиять на популярность трека, однако универсальных объектов слишком много, оставим это на позже, а теперь просмотрим количество пропусков:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7ly_Dx9FpaN"
      },
      "outputs": [],
      "source": [
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f52KNIgxcWCe"
      },
      "source": [
        "удалим все данные, не имеющие нашего целевого признака: popularity, т.к. заполнение его средним значением может пагубно повлиять на качество модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpZjfX5HOtrb"
      },
      "outputs": [],
      "source": [
        "df = df.dropna(subset=['popularity'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2gK7_fbO5UM"
      },
      "outputs": [],
      "source": [
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDgREg09PG1x"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0FmjJ1pQN1b"
      },
      "source": [
        "Имеется относительно небольшое число данных, в которых отсутствует t_*имя атрибута*0, их в целом тоже можно удалить, т.к. без него мы в целом не знаем что-либо о треках из альбома/сингла."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vd3GQBtQgrd"
      },
      "outputs": [],
      "source": [
        "df = df.dropna(subset=['t_val0', 't_sig0', 't_live0','t_tempo0', 't_ins0', 't_acous0','t_key0', 't_dance0'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6qbt2-6Q6sO"
      },
      "outputs": [],
      "source": [
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kTx8yEfVMJr"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i80yeHzfPO6h"
      },
      "source": [
        "Тут вскрывается один минус датасета, из 160 тысяч объектов, где-то у трети нет t_*имя атрибута*1 или 2. По-хорошему, нужно построить различные модели, чтобы оценить различные способы заполнения, однако времени на это нет и мы только в Модуле А, поэтому выбираем один из трех вариантов: заполнить нулями, заполнить средним и удалить данные с пропусками. для этих столбцов выберем заполнение средним и проверим по нескольким графикам не сменилось ли распределение."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xR5HcYALPD3E"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "def test_density_fillmean(df, col: str):\n",
        "    sns.histplot(df[col], kde=True, stat='density')\n",
        "    sns.histplot(df[col].fillna(df[col].mean()), kde=True, stat='density')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Wg13PBLRT4A"
      },
      "outputs": [],
      "source": [
        "test_density_fillmean(df,'t_val1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmgCRNrtRe13"
      },
      "source": [
        "Видим, что распределение изменилось драматически, при заполнении нулём:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7rzO9L7RrEO"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "def test_density_fillzero(df, col: str):\n",
        "    sns.histplot(df[col], kde=True, stat='density')\n",
        "    sns.histplot(df[col].fillna(0), kde=True, stat='density')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqNagEY7RvZ_"
      },
      "outputs": [],
      "source": [
        "test_density_fillzero(df,'t_val2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0B8EI7tJSVeQ"
      },
      "source": [
        "Тоже самое."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4GLgRo_R1T3"
      },
      "outputs": [],
      "source": [
        "import missingno as msno\n",
        "msno.matrix(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rBWHmM2TPKp"
      },
      "outputs": [],
      "source": [
        "df[['t_dur1', 't_val1', 't_dance1']].isnull().corr()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq7zEeg3S50I"
      },
      "source": [
        "Визуально видно, да и по корреляции, что пропуски находятся в одних и тех же местах. Т.е. появляется вариант удалить данные с пропусками, но это УДАЛИТ ТРЕТЬ ДАННЫХ и полностью уберёт синглы, остаётся заполнить все значения нулями, т.к на проверку всех атрибутов времени физически нет :(\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qggs_uMkU06u"
      },
      "outputs": [],
      "source": [
        "for column in df.columns:\n",
        "    if df[column].isna().sum() > 0:\n",
        "        df[column] = df[column].fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jORlYNcGVkq9"
      },
      "outputs": [],
      "source": [
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekNg3fk5XBx-"
      },
      "source": [
        "Пропуски заполнили, теперь переходим к колонке artists. По сути этот столбец конкатинирует в себе несколько артистов и по-хорошему нужно поделить его на несколько столбцов, однако мы не знаем максимальное количество аритстов в релизе(их может быть 10), а также это приведёт к огромному количеству пропусков, так что я решился просто убрать спецсимволы, а затем преобразовать этот признак с помощью TargetEncoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwVsXCgrX_CU"
      },
      "outputs": [],
      "source": [
        "!pip install category-encoders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBHjU32VW_P1"
      },
      "outputs": [],
      "source": [
        "import category_encoders as ce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8cq2WgdX-at"
      },
      "outputs": [],
      "source": [
        "df['artists'] = df['artists'].str.replace('\\W','')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ofsBMZdYSYg"
      },
      "outputs": [],
      "source": [
        "te = ce.TargetEncoder(cols=['artists'])\n",
        "X = df.drop(columns=['popularity'])\n",
        "Y = df['popularity']\n",
        "X = te.fit_transform(X, Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHXcXwMIY3BW"
      },
      "outputs": [],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3RxQdf0Zpga"
      },
      "outputs": [],
      "source": [
        "df = pd.concat([X, Y], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gbqEkdKaFy5"
      },
      "outputs": [],
      "source": [
        "df['artists'].corr(df['popularity'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pglJA2S1dvll"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk6GsMzDiK2N"
      },
      "source": [
        "Видим, что корреляция огромная и в дальнейшем возможно придётся избавляться от этого признака, но что имеем, то имеем. Переходим к преобразованию даты на столбцы."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjUdej-deoJG"
      },
      "outputs": [],
      "source": [
        "date = [i.split('-') for i in df['release_date']]\n",
        "date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MgSPwMed929"
      },
      "outputs": [],
      "source": [
        "def column(matrix, i):\n",
        "    return [row[i]  for row in matrix]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaAyJCOhfgEd"
      },
      "source": [
        "ВИДИМ ЧТО В НЕКОТОРЫХ ДАТАХ ОКАЗЫВАЕТСЯ ТОЛЬКО ГОД, ПОЭТОМУ сохраняем только год."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFL3vhNnaF-Z"
      },
      "outputs": [],
      "source": [
        "df['year'] = column(date, 0)\n",
        "df.drop(['release_date'], axis=1, inplace=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CEx6DFAfxVW"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FXAVVdkgp4l"
      },
      "outputs": [],
      "source": [
        "num_col = []\n",
        "cat_col = []\n",
        "for col in df.columns:\n",
        "    if df.dtypes[col] in ('int64', 'float64'):\n",
        "        num_col.append(col)\n",
        "    else:\n",
        "        cat_col.append(col)\n",
        "num_col"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqyoHDmtiSpU"
      },
      "source": [
        "Строим тепловую карту, масштабируем с помощью MinMax, т.к. не все данные нормальные(было видно ранее) и строим pairplot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-dkpw8rhPqZ"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(data=df[num_col].corr(), vmin=-1, vmax=1, fmt='.2f', cmap=\"crest\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnN_3v0aigqd"
      },
      "source": [
        "Видим, что коррелируют с собой схожие атрибуты с индексами _1 и _2, что логично и как ранее было видно artists с popularity\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cc6bJ8fBg6oa"
      },
      "outputs": [],
      "source": [
        "num_col.remove('popularity')\n",
        "num_col.remove('year')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qubBvi4EhIYI"
      },
      "outputs": [],
      "source": [
        "num_col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiypYNLsgt6L"
      },
      "outputs": [],
      "source": [
        "df['year'] = df['year'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGdL8UB_gLmz"
      },
      "outputs": [],
      "source": [
        "MinMax = MinMaxScaler()\n",
        "df[num_col] = MinMax.fit_transform(df[num_col])\n",
        "df[num_col]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sR2EU_lKj0Fm"
      },
      "source": [
        "Время поджимало, поэтому такой формат: все признаки и их корреляция с целевым"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CN0vWcFbiw12"
      },
      "outputs": [],
      "source": [
        "for i in df.columns:\n",
        "    df.plot.scatter (x = 'popularity', y = i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XVPgwnTikqt"
      },
      "outputs": [],
      "source": [
        "df.to_csv('preprocessed.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSj95dvUZzy1"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HITHqr-QIRAf"
      },
      "source": [
        "#Модуль B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MKFnGMwpa8h"
      },
      "source": [
        "Рассмотрим значения признаков с помощью дерева"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYjA7qSDpkwZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMDaj3rtphtb"
      },
      "outputs": [],
      "source": [
        "X = df.drop(columns=['popularity'])\n",
        "y = df['popularity']\n",
        "DT = DecisionTreeRegressor()\n",
        "features = DT.fit(X,y).feature_importances_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzz-V5PTqKs5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.barh(X.columns, features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6ZWJDHpq60A"
      },
      "source": [
        "Как видим, как и предполагалось значение артистов слишком велико и необходимо убрать этот столбец из датасета F :(. Построим дерево заново без данного признака"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DA4al79Iq3WV"
      },
      "outputs": [],
      "source": [
        "df = df.drop(columns=['artists'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIW5gi5QrJLv"
      },
      "outputs": [],
      "source": [
        "X = df.drop(columns=['popularity'])\n",
        "y = df['popularity']\n",
        "DT = DecisionTreeRegressor()\n",
        "features = DT.fit(X,y).feature_importances_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOVEaCQtqPjo"
      },
      "outputs": [],
      "source": [
        "plt.barh(X.columns, features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rgsfn6tutsJo"
      },
      "outputs": [],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qP3qpUbrc63"
      },
      "source": [
        "Теперь нет выбивающихся признаков и можно переходить к другим способам, будем тестировать модели на BaggingRegressor. Делить выборку будем с помощью train_test_split. Тестовую выборку оставим 0.6,чтобы не тратить много времени на тестирование уменьшения признаков. random_state=22, т.к. моё любимое число :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwEs70Z5r9If"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error,r2_score\n",
        "from math import sqrt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltf6JIi6rcFA"
      },
      "outputs": [],
      "source": [
        "def test(X, y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=22,shuffle=True)\n",
        "    model = BaggingRegressor().fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f'MAE: {mean_absolute_error(y_test, y_pred)}')\n",
        "    print(f'MSE: {mean_squared_error(y_test, y_pred)}')\n",
        "    print(f'RMSE: {sqrt(mean_squared_error(y_test, y_pred))}')\n",
        "    print(f'MAPE: {(mean_absolute_percentage_error(y_test, y_pred))}')\n",
        "    print(f'R^2: {r2_score(y_test, y_pred)}')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVUJPH3Ftmzn"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest\n",
        "for i in range(1, 11):\n",
        "    print(i)\n",
        "    X_new = SelectKBest(k=i).fit_transform(X, y)\n",
        "    test(X_new, y)\n",
        "    print('________________________________')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te4ayuMLu95g"
      },
      "source": [
        "\n",
        "Видим, что качество модели почти перестаёт ухудшаться после 4-5 признаков, выведем признаки в этих количествах\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsG41vsuuvac"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest\n",
        "for i in range(4, 7):\n",
        "    print(i)\n",
        "    KBest = SelectKBest(k=i)\n",
        "    X_new = KBest.fit_transform(X, y)\n",
        "    print(KBest.get_feature_names_out())\n",
        "    test(X_new, y)\n",
        "    print('________________________________')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdJE-kNTuelo"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "for i in range(1, 11):\n",
        "    print(i)\n",
        "    X_new = PCA(n_components=i).fit_transform(X, y)\n",
        "    test(X_new, y)\n",
        "    print('________________________________')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGYUw2P_xWlB"
      },
      "source": [
        "Видим, что оптимальное количество признаков 4-6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOMV6KLWwPv8"
      },
      "source": [
        "Видим, что PCA показал себя гораздо лучше, теперь попробуем RFE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHsrXz3Cvywr"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "tree = DecisionTreeRegressor().fit(X, y)\n",
        "for i in range(4, 7):\n",
        "    print(i)\n",
        "    rfe = RFE(estimator=tree, n_features_to_select=i, step=1).fit(X, y)\n",
        "    X_new = rfe.transform(X)\n",
        "    test(X_new, y)\n",
        "    print('________________________________')\n",
        "rfe = RFE(estimator=tree, n_features_to_select=4, step=1).fit(X, y)\n",
        "X_rfe = pd.DataFrame(rfe.transform(X), columns=rfe.get_feature_names_out())\n",
        "X_rfe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-POzVpryp09"
      },
      "source": [
        "RFE показал результат, схожий с PCA, но при этом сохранил изначальные признаки, поэтому оставим его."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJyku5dZyQP-"
      },
      "outputs": [],
      "source": [
        "rfe = RFE(estimator=tree, n_features_to_select=4, step=1).fit(X, y)\n",
        "X_rfe = pd.DataFrame(rfe.transform(X), columns=rfe.get_feature_names_out())\n",
        "X_rfe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfG3k18o8xZl"
      },
      "outputs": [],
      "source": [
        "rfe_test = RFE(estimator=tree, n_features_to_select=10, step=1).fit(X, y)\n",
        "X_rfe_test = pd.DataFrame(rfe_test.transform(X), columns=rfe_test.get_feature_names_out())\n",
        "test(X_rfe_test, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcFnbapH_Ny7"
      },
      "source": [
        "Проверил, что при сильном увеличении размерности результат не сильно меняется, поэтому оставляю 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XU0x8CwE0RGz"
      },
      "outputs": [],
      "source": [
        "df_visual = pd.concat([X_rfe, y], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJn8WgL10HWM"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(data=df_visual.corr(), vmin=-1, vmax=1, fmt='.2f', cmap=\"crest\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3KiTTps0oei"
      },
      "outputs": [],
      "source": [
        "df_visual.boxplot(column='popularity')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1rIQltv0gWM"
      },
      "source": [
        "Рассмотрим визуализации: коррелирования с целевым признаком нет, поэтому линейные модели вряд-ли покажут себя хорошо, целевой признак сам по себе\n",
        "нормально распределён в диапазоне от 30 до 70, при этом имеет максимум 100 и минимум 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD-W--IYznMK"
      },
      "source": [
        "Сделаем новую разбивку данных, на сей раз, в отличие от теста, оставим на обучение 0.7 данных, чтобы модели показали наилучший результат."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjCHmjqAv0W5"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_rfe, y, test_size=0.3, random_state=22,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXlhj9cZ1Jjh"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6KBtBb6zkZM"
      },
      "outputs": [],
      "source": [
        "params = {'max_depth' : np.arange(20, 100, 20),\n",
        "}\n",
        "model = GridSearchCV(DecisionTreeRegressor(),params).fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(f'MAE: {mean_absolute_error(y_test, y_pred)}')\n",
        "print(f'MSE: {mean_squared_error(y_test, y_pred)}')\n",
        "print(f'RMSE: {sqrt(mean_squared_error(y_test, y_pred))}')\n",
        "print(f'MAPE: {(mean_absolute_percentage_error(y_test, y_pred))}')\n",
        "print(f'R^2: {r2_score(y_test, y_pred)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC-jFchh6RTx"
      },
      "source": [
        "Дерево показало себя +- неплохо, однако неидеально. Для любопытства посмотрим тоже само на PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgdRo1PS6obw"
      },
      "outputs": [],
      "source": [
        "X_PCA = PCA(n_components=5).fit_transform(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eSqDAJP7F0_"
      },
      "outputs": [],
      "source": [
        "X_new_train, X_new_test, y_new_train, y_new_test = train_test_split(X_PCA, y, test_size=0.3, random_state=22,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvMHjxfD7DCn"
      },
      "outputs": [],
      "source": [
        "params = {'max_depth' : np.arange(20, 100, 20),\n",
        "}\n",
        "model = GridSearchCV(DecisionTreeRegressor(),params).fit(X_new_train, y_new_train)\n",
        "y_pred = model.predict(X_new_test)\n",
        "print(f'MAE: {mean_absolute_error(y_new_test, y_pred)}')\n",
        "print(f'MSE: {mean_squared_error(y_new_test, y_pred)}')\n",
        "print(f'RMSE: {sqrt(mean_squared_error(y_new_test, y_pred))}')\n",
        "print(f'MAPE: {(mean_absolute_percentage_error(y_new_test, y_pred))}')\n",
        "print(f'R^2: {r2_score(y_new_test, y_pred)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MwUGjq78YYd"
      },
      "source": [
        "Видим, что разница небольшая, даже немного хуже, так что оставляем признаки из RFE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BbSR27E2bm4"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYulh3gJzkbS"
      },
      "outputs": [],
      "source": [
        "params = {'alpha' : np.arange(0, 1, 0.1),\n",
        "}\n",
        "model = GridSearchCV(Ridge(),params).fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(f'MAE: {mean_absolute_error(y_test, y_pred)}')\n",
        "print(f'MSE: {mean_squared_error(y_test, y_pred)}')\n",
        "print(f'RMSE: {sqrt(mean_squared_error(y_test, y_pred))}')\n",
        "print(f'MAPE: {(mean_absolute_percentage_error(y_test, y_pred))}')\n",
        "print(f'R^2: {r2_score(y_test, y_pred)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luVaXd9t6ZKA"
      },
      "source": [
        "Как и предполагалось линейная модель показала себя плохо и не подходит при данном уменьшении размерности."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHwff39azkhC"
      },
      "outputs": [],
      "source": [
        "!pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7Jd1EAn24SG"
      },
      "outputs": [],
      "source": [
        "from catboost import CatBoostRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibt-Lunq3Dnc"
      },
      "outputs": [],
      "source": [
        "model = CatBoostRegressor().fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(f'MAE: {mean_absolute_error(y_test, y_pred)}')\n",
        "print(f'MSE: {mean_squared_error(y_test, y_pred)}')\n",
        "print(f'RMSE: {sqrt(mean_squared_error(y_test, y_pred))}')\n",
        "print(f'MAPE: {(mean_absolute_percentage_error(y_test, y_pred))}')\n",
        "print(f'R^2: {r2_score(y_test, y_pred)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9X8q84BQ8QNt"
      },
      "source": [
        "CatBoost показал себя неплохо, но не лучше чем дерево."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yZgwSF54b_4"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iME9pvA_KDz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uF0y-274LYu"
      },
      "outputs": [],
      "source": [
        "model = RandomForestRegressor().fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(f'MAE: {mean_absolute_error(y_test, y_pred)}')\n",
        "print(f'MSE: {mean_squared_error(y_test, y_pred)}')\n",
        "print(f'RMSE: {sqrt(mean_squared_error(y_test, y_pred))}')\n",
        "print(f'MAPE: {(mean_absolute_percentage_error(y_test, y_pred))}')\n",
        "print(f'R^2: {r2_score(y_test, y_pred)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzjPEtyS_Yev"
      },
      "outputs": [],
      "source": [
        "params = {'n_estimators' : np.arange(50, 80, 10),\n",
        "           'max_depth': np.arange(10, 25, 5)}\n",
        "model = RandomizedSearchCV(RandomForestRegressor(), params).fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(f'MAE: {mean_absolute_error(y_test, y_pred)}')\n",
        "print(f'MSE: {mean_squared_error(y_test, y_pred)}')\n",
        "print(f'RMSE: {sqrt(mean_squared_error(y_test, y_pred))}')\n",
        "print(f'MAPE: {(mean_absolute_percentage_error(y_test, y_pred))}')\n",
        "print(f'R^2: {r2_score(y_test, y_pred)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-R4jX-hDCdE"
      },
      "source": [
        "RandomizedSearch не успел :("
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZeB133P9Nie"
      },
      "outputs": [],
      "source": [
        "choosen_model = RandomForestRegressor().fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dTUB5wtPiGK"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Chz1PXG9DRgA"
      },
      "outputs": [],
      "source": [
        "with open('model.pkl', 'wb') as dump_out:\n",
        "    pickle.dump(choosen_model, dump_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TB8TkkL8k2e"
      },
      "source": [
        "Случайный лес показал себя лучше всех, поэтому в следующих этапах будем использовать RandomForest, его и сохраним"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqBh2p_EIXak"
      },
      "source": [
        "#Модуль C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDQ4iixUPsZn",
        "outputId": "21f16a03-4e1c-4d04-b68f-8e1ed48874a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m799.2/799.2 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit\n",
        "!pip install -q streamlit-option-menu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEI3tUXjP1EU",
        "outputId": "e1d12644-f1c2-4777-f6fb-c7b51a2189b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[34;40mnotice\u001b[0m\u001b[35m\u001b[0m created a lockfile as package-lock.json. You should commit this file.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "added 22 packages from 22 contributors and audited 22 packages in 2.144s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found \u001b[92m0\u001b[0m vulnerabilities\n",
            "\n",
            "\u001b[K\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PhdQDICRbv8"
      },
      "source": [
        "Создадим MinMaxPreprocesser, обученный на изначальных данных, для использования в Dash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2C2cQV93Ra7k"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('albums.csv', delimiter='|', parse_dates=['release_date'])\n",
        "df = df.dropna(subset=['t_val0', 't_sig0', 't_live0','t_tempo0', 't_ins0', 't_acous0','t_key0', 't_dance0'])\n",
        "df = df.dropna(subset=['popularity'])\n",
        "needed_data = df[[\"total_tracks\", \"t_dur0\", \"t_speech0\", \"t_acous0\"]]\n",
        "MinMax = MinMaxScaler()\n",
        "MinMax.fit(needed_data)\n",
        "with open('MinMax.pkl', 'wb') as dump_out:\n",
        "    pickle.dump(MinMax, dump_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7v6I6BeaDjL"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('albums.csv', delimiter='|', parse_dates=['release_date'])\n",
        "df.head(10).to_csv('test.csv', index=False, sep='|')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cnd3GsAFPc7y",
        "outputId": "9e4b23f0-47b5-45fb-b975-8515ceb43403"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import pickle\n",
        "import seaborn as sns\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "from streamlit_option_menu import option_menu\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "with open('model.pkl', 'rb') as f:\n",
        "    random_forest = pickle.load(f)\n",
        "\n",
        "with open('MinMax.pkl', 'rb') as f:\n",
        "    num_preprocessor = pickle.load(f)\n",
        "\n",
        "def preprocessing(df):\n",
        "    df = df.dropna(subset=['total_tracks', 't_dur0', 't_speech0', 't_acous0'])\n",
        "    df[['total_tracks', 't_dur0', 't_speech0', 't_acous0']] = num_preprocessor.transform(df[['total_tracks', 't_dur0', 't_speech0', 't_acous0']])\n",
        "    return df[['total_tracks', 't_dur0', 't_speech0', 't_acous0']]\n",
        "\n",
        "def upload():\n",
        "    uploaded_file = st.file_uploader(\"Выберите файл .csv (delimiter = '|')\")\n",
        "\n",
        "    if uploaded_file:\n",
        "        upload_data = pd.read_csv(uploaded_file, sep=\"|\")\n",
        "        upload_data = preprocessing(upload_data)\n",
        "\n",
        "    if st.button(\"Получить предсказание\", key='1'):\n",
        "        if not uploaded_file:\n",
        "            st.warning('Сначала нужно загрузить данные!', icon=\"⚠️\")\n",
        "        else:\n",
        "            y_pred = random_forest.predict(upload_data)\n",
        "            upload_data['popularity'] = y_pred\n",
        "            st.write(upload_data)\n",
        "            st.download_button(\"Сохранить результат\", upload_data.to_csv(index=False, encoding='utf-8', sep='|'), \"file.csv\", \"text/csv\", key='download-csv')\n",
        "\n",
        "def input():\n",
        "    edited_df = st.data_editor(pd.DataFrame([{\n",
        "        \"total_tracks\": \"\",\n",
        "        \"t_dur0\": \"\",\n",
        "        \"t_speech0\": \"\",\n",
        "        \"t_acous0\": \"\",\n",
        "    }]), num_rows=\"dynamic\", use_container_width=False, width = 1000)\n",
        "\n",
        "    if st.button(\"Получить предсказание\", key='2'):\n",
        "        try:\n",
        "            edited_df = preprocessing(edited_df)\n",
        "            y_pred = random_forest.predict(edited_df)\n",
        "            edited_df['popularity'] = y_pred\n",
        "            st.write(edited_df)\n",
        "            st.download_button(\"Сохранить результат\", edited_df.to_csv(index=False, encoding='utf-8',sep='|'), \"file.csv\", \"text/csv\", key='download-csv')\n",
        "        except:\n",
        "            st.warning('Некорректные данные!', icon=\"⚠️\")\n",
        "\n",
        "def prediction():\n",
        "    tab1, tab2 = st.tabs([\"Загрузка файла .csv\", \"Ручной ввод значений\"])\n",
        "\n",
        "    with tab1:\n",
        "        upload()\n",
        "\n",
        "    with tab2:\n",
        "        input()\n",
        "\n",
        "INFO = r\"\"\"\n",
        "# Добро пожаловать в приложение для предсказывания популярности вашего трека!\n",
        "\n",
        "## Описание входных данных (объекты)\n",
        "\n",
        "Для предсказания вам необходимы не все данные, а только перечисленные ниже. Все они являются числовыми\n",
        "\n",
        "|  Название столбца  | Описание                                   |\n",
        "| ------------------ | ------------------------------------------ |\n",
        "| total_tracks       | Количество треков в релизе                 |\n",
        "| t_dur0             | Длительность 1 трека в релизе  (в мс)      |\n",
        "| t_speech0          | Соотношение слов в треке(от 0 до 1)              |\n",
        "| t_acous0           | Насколько звучание близко к акустике(от 0 до 1)  |\n",
        "\n",
        "\n",
        "## Описание выходных данных (предсказания)\n",
        "\n",
        "Предсказываться будет значение $popularity \\in $ ($0, 100$) - потенциальная популяность вашего трека.\n",
        "\n",
        "Таким образом, решается задача регресссии.\n",
        "\n",
        "## Модель\n",
        "\n",
        "В предсказании используется модель RandomForest, которая показала наилучшие результаты на выделенном подпространстве признаков.\n",
        "\n",
        "## Визуализации\n",
        "\n",
        "Предоставлена визуализация heatmap усеченного пространства признаков.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def data_description():\n",
        "    st.markdown(INFO)\n",
        "\n",
        "def visualisation():\n",
        "    df = pd.read_csv('C[03]_A_preprocessed.csv', delimiter=',', encoding = 'utf-8')\n",
        "    df_col = df[['total_tracks', 't_dur0', 't_speech0', 't_acous0', 'popularity']]\n",
        "    fig, ax = plt.subplots()\n",
        "    sns.heatmap(df_col.corr(), ax=ax)\n",
        "    st.write('Тепловая карта признаков')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "\n",
        "menu = {\n",
        "    \"app\":\n",
        "    {\n",
        "        \"Описание данных\": data_description,\n",
        "        \"Предсказание\": prediction,\n",
        "        \"Визуализация\": visualisation,\n",
        "    }\n",
        "}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    with st.sidebar:\n",
        "            selected = option_menu(\"Меню\", [\"Описание данных\", 'Визуализация', 'Предсказание'],\n",
        "                icons=['info-circle', 'bar-chart', 'tag'], menu_icon=\"cast\", default_index=0)\n",
        "\n",
        "    menu[\"app\"][selected]()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muZAYxmlT2VV"
      },
      "outputs": [],
      "source": [
        "!streamlit run app.py &>/content/logs.txt & curl ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3T4jXEaT31k"
      },
      "outputs": [],
      "source": [
        "!npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}